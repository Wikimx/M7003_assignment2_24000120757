# -*- coding: utf-8 -*-
"""M7003_assignment2_24000120757

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xTXdB0bME6eoEFfgg_IbyKrCv07OegDm

Install libraries we will be using
"""

!pip install torch==2.0.1 torchvision==0.15.2 transformers==4.31.0

"""Upload documents and make structure data"""

import pandas as pd
import re

def process_transcription(file_name):
    """
    Process a transcription file to extract structured data.
    """
    try:
        with open(file_name, 'r', encoding='utf-8') as f:
            lines = f.readlines()
    except FileNotFoundError:
        raise FileNotFoundError("The specified file was not found.")

    # Extract metadata from the file name
    match = re.match(r"Plaza (.+) Edades (.+) NSE (.+)\.txt", file_name.split('/')[-1])
    if not match:
        raise ValueError("File name format is incorrect. Expected format: 'Plaza [Name] Edades [Range] NSE [Value].txt'")

    plaza, ages, nse = match.groups()

    data = []
    # Adjusted pattern to handle both mm:ss and hh:mm:ss
    pattern = r"^(\d{1,2}:\d{2}(?::\d{2})?)\s+([^:]+):\s+(.+)"

    for line in lines:
        match = re.match(pattern, line)
        if match:
            time, name, participation = match.groups()

            # Normalize mm:ss to hh:mm:ss
            if len(time.split(':')) == 2:
                time = f"00:{time}"

            data.append({
                'Time': time,
                'Name': name,
                'Participation': participation.strip(),
                'Plaza': plaza,
                'NSE': nse,
                'Ages': ages
            })

    if not data:
        raise ValueError("No valid data extracted from the transcription file.")

    df = pd.DataFrame(data)
    return df

# List of file names
file_names = [
    "/content/Assignment 2/data/raw/Plaza CDMX Edades 20 - 30 años NSE C-.txt",
    "/content/Assignment 2/data/raw/Plaza CDMX Edades 35 - 55 años NSE C+.txt",
    "/content/Assignment 2/data/raw/Plaza GDL Edades 20 - 30 años NSE C-.txt",
    "/content/Assignment 2/data/raw/Plaza GDL Edades 35 - 55 años NSE C+.txt",
    "/content/Assignment 2/data/raw/Plaza MTY Edades 20 - 30 años NSE C+.txt",
    "/content/Assignment 2/data/raw/Plaza MTY Edades 35 - 55 años NSE C-.txt"
]

all_dataframes = []

try:
    for file_name in file_names:
        # Process each transcription file
        df = process_transcription(file_name)
        all_dataframes.append(df)

    # Combine all dataframes into one
    combined_df = pd.concat(all_dataframes, ignore_index=True)

    # Display the first rows of the combined DataFrame
    print(combined_df.head())

    # Save the combined DataFrame to a CSV file
    combined_df.to_csv("combined_transcriptions.csv", index=False, encoding='utf-8')
    print("All files processed and saved successfully.")

except Exception as e:
    print(f"An error occurred: {e}")

"""Assign groups"""

# Load the cleaned DataFrame
cleaned_df = pd.read_csv("/content/Assignment 2/data/combined_transcriptions.csv")

# Assign group numbers based on Plaza, NSE, and Ages
unique_groups = cleaned_df.groupby(['Plaza', 'NSE', 'Ages']).ngroup()
cleaned_df['Group'] = unique_groups

# Save the updated DataFrame
cleaned_df.to_csv("transcriptions_with_groups.csv", index=False, encoding='utf-8')

print("Group column added based on Plaza, NSE, and Ages and saved to 'transcriptions_with_groups.csv'.")

"""Hash participants and add moderators

"""

import hashlib

# List of known moderators
moderators = ["Karime Galicia", "Mario Juárez", "Natalia Rodríguez", "Diego De Alba Montes"]

def anonymize_and_flag(df):
    """
    Add a column to flag if the person is a Moderator or Participant,
    and anonymize the names of participants.
    """
    # Create a column to flag if the row corresponds to a Moderator
    df['Role'] = df['Name'].apply(lambda x: 'Moderator' if x in moderators else 'Participant')

    # Anonymize names for participants
    def anonymize_name(name, role):
        if role == 'Participant':
            return hashlib.sha256(name.encode()).hexdigest()[:10]  # Return first 10 characters of hash
        return name  # Keep moderator names unchanged

    df['Anonymized_Name'] = df.apply(lambda row: anonymize_name(row['Name'], row['Role']), axis=1)

    return df

# Load the previously saved DataFrame
combined_df = pd.read_csv("/content/Assignment 2/data/transcriptions_with_groups.csv")

# Process the DataFrame to flag roles and anonymize participant names
processed_df = anonymize_and_flag(combined_df)

# Save the processed DataFrame to a new CSV file
processed_df.to_csv("transcriptions_with_moderators_and_groups.csv", index=False, encoding='utf-8')

print("Names anonymized and roles flagged successfully.")

"""Clean the text for basic analysis"""

!pip install nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
import nltk

# Download required NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
# Download the 'punkt_tab' resource for Spanish sentence tokenization
nltk.download('punkt_tab') # This line was added

def clean_text(participation):
    """
    Clean and preprocess the participation text.
    """
    # Remove non-alphanumeric characters, except spaces
    text = re.sub(r'[^\w\s]', '', participation)
    # Normalize whitespace
    text = re.sub(r'\s+', ' ', text)
    # Convert to lowercase
    text = text.lower()
    # Tokenize text
    tokens = word_tokenize(text, language='spanish')
    # Remove stopwords (in Spanish)
    spanish_stopwords = set(stopwords.words('spanish'))
    cleaned_tokens = [token for token in tokens if token not in spanish_stopwords]
    # Join tokens back into a string
    return ' '.join(cleaned_tokens)

def preprocess_data(df):
    """
    Apply text cleaning to the participation column while keeping the time.
    """
    df['Cleaned_Participation'] = df['Participation'].apply(clean_text)
    return df

# Load the previously processed DataFrame
processed_df = pd.read_csv("/content/Assignment 2/data/transcriptions_with_moderators_and_groups.csv")

# Clean the participation column
cleaned_df = preprocess_data(processed_df)

# Save the cleaned DataFrame to a new CSV file
cleaned_df.to_csv("cleaned_transcriptions.csv", index=False, encoding='utf-8')

print("Participation text cleaned and saved successfully.")

"""Clean the text for analysis with BETO"""

# Function to clean text without removing stopwords
def clean_text_preserve_stopwords(text):
    if isinstance(text, str):
        # Remove URLs
        text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)
        # Remove irrelevant special characters (except basic punctuation)
        text = re.sub(r"[^\w\s,.¿?!¡]", '', text)
        # Convert text to lowercase
        text = text.lower()
        return text.strip()  # Remove extra spaces
    return text

# Load the file with the original participation data
file_path = "/content/Assignment 2/data/cleaned_transcriptions.csv"
df = pd.read_csv(file_path)

# Apply text cleaning while preserving stopwords
df['Cleaned_for_Beto'] = df['Participation'].apply(clean_text_preserve_stopwords)

# Save the updated file
output_path = "/content/Assignment 2/data/transcriptions_with_cleaned_Beto.csv"
df.to_csv(output_path, index=False, encoding='utf-8-sig')

print(f"File with basic text cleaning that preserves stopwords saved at '{output_path}'.")

"""Themes aplication to participation"""

import pandas as pd
import re

# Final adjusted thematic categories in English (excluding proper nouns)
thematic_categories = {
    "General opinions about the country": ["país", "méxico", "nación", "gobierno"],
    "Economic problems": ["desigualdad", "economía", "crisis", "jodidos"],
    "Security problems": ["crimen", "violencia", "inseguridad", "narco", "robo", "asalto"],
    "Corruption problems": ["corrupción", "impunidad", "transa", "mordida"],
    "Post-electoral mood": ["elección", "fraude", "optimismo", "desilusión", "votación", "legitimidad", "sentimiento", "emoción", "elecciones"],
    "Morena": ["obrador", "amlo", "morena", "moreno", "guinda", "andres manuel lopez"],
    "Movimiento Ciudadano": ["movimiento ciudadano", "naranja", "canción"],
    "PRIAN": ["pri", "anaya", "pan", "prian", "xochitl"],
    "Sheinbaum": ["claudia", "sheinbaum", "presidenta"],
    "Samuel García": ["samuel", "garcia", "nuevo leon", "gobernador"],
    "Maynez": ["maynez", "maines", "dirigencia", "liderazgo", "presidencia del partido", "sí saben quién"]
}

# Function to classify participation into multiple themes
def classify_multiple_thematic_categories(text, categories):
    assigned_themes = set()
    if isinstance(text, str):
        text = text.lower()  # Normalize text to lowercase
        for theme, keywords in categories.items():
            for keyword in keywords:
                if re.search(rf'\b{keyword}\b', text):
                    assigned_themes.add(theme)
    return ", ".join(assigned_themes) if assigned_themes else "Unclassified"

# Apply thematic classification
def classify_thematic_categories(group):
    group['Thematic_Categories'] = group['Cleaned_for_Beto'].apply(
        lambda text: classify_multiple_thematic_categories(text, thematic_categories)
    )
    return group

# Updated contextualization rule for "Maynez"
def classify_maynez_with_context(group):
    for index, row in group.iterrows():
        # Check if the participation contains terms specific to Maynez
        if isinstance(row['Cleaned_for_Beto'], str):
            text = row['Cleaned_for_Beto'].lower()

            # High-priority keywords for Maynez
            keywords = ["maynez", "maines", "movimiento ciudadano"]
            leadership_terms = ["dirigente", "presidente", "liderazgo", "presidencia del partido"]

            # Check if any high-priority keyword exists
            if any(keyword in text for keyword in keywords):
                current_themes = set(row['Thematic_Categories'].split(", ")) if isinstance(row['Thematic_Categories'], str) else set()
                current_themes.add("Maynez")
                group.at[index, 'Thematic_Categories'] = ", ".join(current_themes)

            # Check for leadership context
            if "movimiento ciudadano" in text and any(term in text for term in leadership_terms):
                current_themes = set(row['Thematic_Categories'].split(", ")) if isinstance(row['Thematic_Categories'], str) else set()
                current_themes.add("Maynez")
                group.at[index, 'Thematic_Categories'] = ", ".join(current_themes)
    return group

# Propagate moderator themes to participant responses within the same group and plaza
def propagate_moderator_themes(group):
    current_theme = None
    for index, row in group.iterrows():
        if row['Role'] == 'Moderator' and row['Thematic_Categories'] != "Unclassified":
            current_theme = row['Thematic_Categories']
        elif row['Role'] == 'Participant' and current_theme:
            if row['Thematic_Categories'] == "Unclassified":
                group.at[index, 'Thematic_Categories'] = current_theme
            else:
                existing_themes = set(row['Thematic_Categories'].split(", "))
                new_themes = existing_themes.union(set(current_theme.split(", ")))
                group.at[index, 'Thematic_Categories'] = ", ".join(new_themes)
    return group

# Load the cleaned transcriptions file
data_path = "/content/Assignment 2/data/transcriptions_with_cleaned_Beto.csv"
cleaned_df = pd.read_csv(data_path, encoding='utf-8')

# Sort data chronologically within groups
cleaned_df = cleaned_df.sort_values(by=['Group', 'Plaza', 'Time'])

# Apply thematic classification
cleaned_df = cleaned_df.groupby(['Group', 'Plaza'], group_keys=False).apply(classify_thematic_categories)

# Apply the updated "Maynez" rule
cleaned_df = cleaned_df.groupby(['Group', 'Plaza'], group_keys=False).apply(classify_maynez_with_context)

# Propagate themes exclusively for moderators
cleaned_df = cleaned_df.groupby(['Group', 'Plaza'], group_keys=False).apply(propagate_moderator_themes)

# Save the results to a new CSV file
output_path = "multi_thematic_classified_transcriptions_reviewed_with_updated_maynez.csv"
cleaned_df.to_csv(output_path, index=False, encoding='utf-8-sig')

print(f"Multi-thematic classification with updated rules completed, saved to '{output_path}'.")

"""Sentiment analisis with Beto and transformers"""

import pandas as pd
from transformers import pipeline
from tqdm.notebook import tqdm  # For showing progress

# Enable tqdm to work properly in Jupyter/Colab environments
tqdm.pandas()

# Load the file with processed text
file_path = "/content/Assignment 2/data/multi_thematic_classified_transcriptions_reviewed_with_updated_maynez.csv"
df = pd.read_csv(file_path)

# Load the BETO model for sentiment analysis
sentiment_analyzer = pipeline(
    "text-classification",
    model="finiteautomata/beto-sentiment-analysis",
    tokenizer="finiteautomata/beto-sentiment-analysis",
)

# Function to calculate numeric sentiment using BETO
def calculate_beto_sentiment(text):
    if isinstance(text, str):
        result = sentiment_analyzer(text[:1024])  # Process up to 1024 characters
        sentiment_label = result[0]['label']
        sentiment_score = result[0]['score']

        # Map sentiment labels to numeric values
        if sentiment_label == "NEG":
            return -sentiment_score  # Negative sentiment
        elif sentiment_label == "POS":
            return sentiment_score  # Positive sentiment
        return 0  # Neutral sentiment
    return None

# Exclude sentiment analysis for moderators
def exclude_moderators_sentiment_analysis(df):
    # Create a condition to filter participants only
    participants_df = df[df['Role'] == 'Participant']  # Only analyze participants
    # Process sentiment analysis for participants
    participants_df['BETO_Numeric_Sentiment_Processed'] = participants_df['Cleaned_for_Beto'].progress_apply(calculate_beto_sentiment)
    # Merge results back into the original DataFrame
    df = df.merge(
        participants_df[['BETO_Numeric_Sentiment_Processed']],
        left_index=True,
        right_index=True,
        how='left'
    )
    return df

# Apply the exclusion and sentiment analysis
df = exclude_moderators_sentiment_analysis(df)

# Save the results to a CSV file
output_path = "/content/Assignment 2/data/participants_beto_sentiment_processed.csv"
df.to_csv(output_path, index=False, encoding='utf-8-sig')

print(f"Sentiment analysis with BETO completed (excluding moderators) and saved to '{output_path}'.")

"""Sentiment analysis by differnt agrupations"""

# Load the file with processed sentiment results
file_path = "/content/Assignment 2/data/participants_beto_sentiment_processed.csv"
df = pd.read_csv(file_path)

# Create an empty DataFrame to store the results
all_results = pd.DataFrame()

# Calculate the average sentiment for each main category
categories = {
    "Group": "Group",
    "Plaza": "Plaza",
    "NSE": "NSE",
    "Ages": "Ages",
    "Thematic_Categories": "Thematic_Categories"
}

for category_name, column_name in categories.items():
    # Group by the category and calculate the average sentiment
    category_sentiment = df.groupby(column_name)['BETO_Numeric_Sentiment_Processed'].mean().reset_index()
    category_sentiment.rename(columns={'BETO_Numeric_Sentiment_Processed': 'Avg_Sentiment'}, inplace=True)
    category_sentiment['Category_Type'] = category_name  # Add a column to identify the category type

    # Combine the results into a single DataFrame
    all_results = pd.concat([all_results, category_sentiment], ignore_index=True)

# Complete list of unique thematic categories
unique_thematics = [
    "General opinions about the country",
    "Economic problems",
    "Security problems",
    "Corruption problems",
    "Post-electoral mood",
    "Morena",
    "Movimiento Ciudadano",
    "PRIAN",
    "Sheinbaum",
    "Samuel García",
    "Maynez"
]

# Create a column for each thematic category, marking whether it is present or not
for thematic in unique_thematics:
    df[thematic] = df['Thematic_Categories'].str.contains(thematic, na=False).astype(int)

# Calculate the average sentiment for each thematic category (including combinations)
thematic_sentiments = []
for thematic in unique_thematics:
    # Filter participations containing the thematic category
    filtered_df = df[df[thematic] == 1]
    avg_sentiment = filtered_df['BETO_Numeric_Sentiment_Processed'].mean()
    thematic_sentiments.append({"Thematic_Category": thematic, "Avg_Sentiment": avg_sentiment, "Category_Type": "Thematic_Categories"})

# Convert the thematic results into a DataFrame
thematic_sentiments_df = pd.DataFrame(thematic_sentiments)

# Combine thematic results with general category results
all_results = pd.concat([all_results, thematic_sentiments_df], ignore_index=True)

# Save all results into a single CSV file
output_path = "/content/Assignment 2/data/aggregated_sentiments_combined.csv"
all_results.to_csv(output_path, index=False, encoding='utf-8-sig')

# Display a preview of the results
print("Aggregated results by category and combined thematic categories:")
print(all_results.head())

print(f"Consolidated file saved at '{output_path}'.")

import pandas as pd

# Load the file with processed results
file_path = "/content/Assignment 2/data/participants_beto_sentiment_processed.csv"
df = pd.read_csv(file_path)

# Add a column to mark if "Maynez" is present (including combinations)
df['Contains_Maynez'] = df['Thematic_Categories'].str.contains("Maynez", na=False).astype(int)

# Add a column to mark if "Maynez" is the only thematic category
df['Only_Maynez'] = df['Thematic_Categories'].apply(
    lambda x: x.strip() == "Maynez" if isinstance(x, str) else False
).astype(int)

# Add a column to mark if "Movimiento Ciudadano" is present (including combinations)
df['Contains_Movimiento_Ciudadano'] = df['Thematic_Categories'].str.contains("Movimiento Ciudadano", na=False).astype(int)

# Add a column to mark if "Movimiento Ciudadano" is the only thematic category
df['Only_Movimiento_Ciudadano'] = df['Thematic_Categories'].apply(
    lambda x: x.strip() == "Movimiento Ciudadano" if isinstance(x, str) else False
).astype(int)

# Define specific combinations for analysis
combinations = [
    {"Ages": "20 - 30 años", "NSE": "C-", "Thematic": "Maynez"},
    {"Ages": "20 - 30 años", "NSE": "C+", "Thematic": "Maynez"},
    {"Ages": "35 - 55 años", "NSE": "C-", "Thematic": "Maynez"},
    {"Ages": "35 - 55 años", "NSE": "C+", "Thematic": "Maynez"},
    {"Ages": "20 - 30 años", "NSE": "C-", "Thematic": "Movimiento Ciudadano"},
    {"Ages": "20 - 30 años", "NSE": "C+", "Thematic": "Movimiento Ciudadano"},
    {"Ages": "35 - 55 años", "NSE": "C-", "Thematic": "Movimiento Ciudadano"},
    {"Ages": "35 - 55 años", "NSE": "C+", "Thematic": "Movimiento Ciudadano"}
]

# Calculate results for each combination
results = []
for combo in combinations:
    thematic = combo['Thematic']
    contains_col = f"Contains_{thematic.replace(' ', '_')}"
    only_col = f"Only_{thematic.replace(' ', '_')}"

    # Filter for participations where the thematic is present (combined or alone)
    filtered_df_contains = df[(df['Ages'] == combo['Ages']) &
                              (df['NSE'] == combo['NSE']) &
                              (df[contains_col] == 1)]

    # Filter for participations where the thematic is the only category
    filtered_df_only = df[(df['Ages'] == combo['Ages']) &
                          (df['NSE'] == combo['NSE']) &
                          (df[only_col] == 1)]

    # Calculate the average sentiment
    avg_sentiment_contains = filtered_df_contains['BETO_Numeric_Sentiment_Processed'].mean()
    avg_sentiment_only = filtered_df_only['BETO_Numeric_Sentiment_Processed'].mean()

    # Save the results
    results.append({
        "Ages": combo['Ages'],
        "NSE": combo['NSE'],
        "Thematic": combo['Thematic'],
        "Avg_Sentiment_Contains": avg_sentiment_contains,
        "Avg_Sentiment_Only": avg_sentiment_only
    })

# Convert results to a DataFrame
results_df = pd.DataFrame(results)

# Save the results to a single CSV file
output_path = "specific_combinations_sentiment_detailed.csv"
results_df.to_csv(output_path, index=False, encoding='utf-8-sig')

# Display the results
print("Sentiment results for specific combinations:")
print(results_df)

"""Words Cloud"""

# Install necessary libraries
!pip install wordcloud matplotlib pandas

# Import libraries
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Load the CSV file # This line was added to load the data into 'data'
file_path = '/content/Assignment 2/data/participants_beto_sentiment_processed.csv'
data = pd.read_csv(file_path)

# Define a list of words to remove
words_to_remove = ["pues", "entonces", "creo", "así", "ahí", "ahorita", "voy", "ahora",
                   "hace", "si", "va", "siento", "luego", "aquí", "años", "mucha",
                   "mucho", "cosa", "elecciones", "bueno", "bien", "partido", "morena", "pri", "pan"
                   "movimiento ciudadano", "verdad", "movimiento", "ciudadano", "ser", "sé", "ver", "veo"
                   "hacer", "dice", "cómo", "hacer", "cosas", "solo", "claudia", "quiero", "puede"
                   "partidos", "sino", "gusta", "tema", "veo", "digo", "okay", "decir", "´nar", "ok" ]

# Function to clean a text by removing specific words
def clean_text(text, words):
    # Split the text into words and filter out the words to remove
    cleaned_words = [word for word in str(text).split() if word.lower() not in words]
    # Join the words back into a single string
    return ' '.join(cleaned_words)

# Apply the cleaning function to the 'Cleaned_Participation' column
data['Cleaned_Participation'] = data['Cleaned_Participation'].apply(lambda x: clean_text(x, words_to_remove))

# Verify the cleaned column
print(data['Cleaned_Participation'].head())

# Save the cleaned data to a new file with proper encoding
data.to_csv('/content/Assignment 2/data/participants_beto_sentiment_processed_cleaned.csv', index=False, encoding='utf-8-sig')

# Save the cleaned data to a new file with proper encoding
data.to_csv('/content/Assignment 2/data/participants_beto_sentiment_processed_cleaned.csv', index=False, encoding='utf-8-sig')

# Install necessary libraries
!pip install wordcloud matplotlib pandas

# Import libraries
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Load the CSV file
file_path = '/content/Assignment 2/data/participants_beto_sentiment_processed_cleaned.csv'
data = pd.read_csv(file_path)

# Define the target categories
target_categories = [
    "General opinions about the country",
    "Economic problems",
    "Security problems",
    "Corruption problems",
    "Post-electoral mood",
    "Morena",
    "Movimiento Ciudadano",
    "PRIAN",
    "Sheinbaum",
    "Samuel García",
    "Maynez"
]

# Check the columns in the dataset
print(data.columns)

# Generate a word cloud for each target category
if 'Thematic_Categories' in data.columns and 'Cleaned_Participation' in data.columns:
    for category in target_categories:
        # Filter rows where the category appears in 'Thematic_Categories'
        category_data = data[data['Thematic_Categories'].str.contains(category, na=False)]

        # Combine all words in the 'Cleaned_Participation' column for this category
        text = ' '.join(category_data['Cleaned_Participation'].dropna())

        # Generate the word cloud
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

        # Display the word cloud
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.title(f'Word Cloud for Category: {category}', fontsize=14)
        plt.axis('off')
        plt.show()
else:
    print("Make sure the columns 'Thematic_Categories' and 'Cleaned_Participation' exist in the CSV file.")

"""Topic modeling LDA

Lematization
"""

# Install spaCy and download the Spanish model
!pip install spacy
!python -m spacy download es_core_news_sm

# Import necessary libraries
import spacy
import unicodedata

# Load the Spanish language model
nlp = spacy.load('es_core_news_sm')

# Function to normalize text (remove special characters and preserve accents)
def normalize_text(text):
    if isinstance(text, str):
        return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')
    return text

# Function to lemmatize text in Spanish with improved handling
def lemmatize_text(text):
    """
    Lemmatizes the given text using spaCy's Spanish model.
    Removes stopwords and punctuation.
    Handles unusual casing.
    """
    if isinstance(text, str):  # Check if text is a string
        text = text.lower()  # Convert to lowercase to aid lemmatization
        doc = nlp(text)
        lemmas = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]

        # Manually handle specific cases if needed
        for i, lemma in enumerate(lemmas):
            if lemma == "apagadito":  # Example of a word needing manual correction
                lemmas[i] = "apagar"  # Replace with desired lemma

        return ' '.join(lemmas)
    return ""  # Return an empty string for non-string values

# Normalize and apply lemmatization to the 'Cleaned_Participation' column
if 'Cleaned_Participation' in data.columns:
    # Normalize text
    data['Cleaned_Participation'] = data['Cleaned_Participation'].dropna().apply(normalize_text)
    # Lemmatize text, handling non-string values
    data['Lemmatized_Text'] = data['Cleaned_Participation'].apply(lemmatize_text)
else:
    print("The column 'Cleaned_Participation' does not exist in the dataset.")

# Save the updated DataFrame to a CSV file with proper encoding
output_file_path = '/content/lemmatized_data.csv'
data.to_csv(output_file_path, index=False, encoding='utf-8')

# Verify the new column 'Lemmatized_Text'
print(data['Lemmatized_Text'].head())
print(f"Processed file saved at: {output_file_path}")

# Path to the lemmatized data file
file_path = '/content/Assignment 2/data/lemmatized_data.csv'

# Load the data into a DataFrame
data = pd.read_csv(file_path)

# Ensure the 'Lemmatized_Text' column exists
if 'Lemmatized_Text' in data.columns:
    print("Column 'Lemmatized_Text' found. Proceeding with LDA.")
else:
    print("Column 'Lemmatized_Text' not found in the dataset. Please check the file.")

# Verify a sample of the lemmatized text
print(data['Lemmatized_Text'].head())

# Ensure all values in 'Lemmatized_Text' are strings
data['Lemmatized_Text'] = data['Lemmatized_Text'].fillna('')  # Replace NaN values with empty strings
data['Lemmatized_Text'] = data['Lemmatized_Text'].astype(str)  # Convert all values to strings

# Tokenize the lemmatized text
texts = [doc.split() for doc in data['Lemmatized_Text']]  # Split each text into tokens

# Import the Dictionary class from gensim.corpora
from gensim.corpora import Dictionary
# Create a dictionary and a corpus
dictionary = Dictionary(texts)  # Map each unique word to an ID
corpus = [dictionary.doc2bow(text) for text in texts]  # Convert texts to a bag-of-words format


# Import the LdaModel class from gensim.models
from gensim.models import LdaModel
# Train the LDA model
num_topics = 5  # Adjust the number of topics as needed
lda_model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=10)

# Print the generated topics
print("Generated topics:")
for idx, topic in lda_model.print_topics(-1):  # Print all topics
    print(f"Topic {idx}: {topic}")

# Define the path for the output file
output_file = '/content/lda_topics.csv'

# Extract topics into a structured format
topics = []
for idx, topic in lda_model.print_topics(-1):  # Loop through all topics
    topics.append([idx, topic])  # Append topic index and its content

# Convert topics into a DataFrame
import pandas as pd
topics_df = pd.DataFrame(topics, columns=['Topic_Index', 'Topic_Content'])

# Save the DataFrame to a CSV file
topics_df.to_csv(output_file, index=False, encoding='utf-8')

print(f"Topics saved to {output_file}")

"""Visualisation

"""

# Install pyLDAvis if not already installed
!pip install pyLDAvis

# Import pyLDAvis
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis

# Enable notebook visualization (for Colab/Jupyter)
pyLDAvis.enable_notebook()

# Prepare the visualization
vis = gensimvis.prepare(lda_model, corpus, dictionary)

# Display the visualization
pyLDAvis.display(vis)

# Import pyLDAvis
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis

# Prepare the visualization
vis = gensimvis.prepare(lda_model, corpus, dictionary)

# Save the visualization as an HTML file
pyLDAvis.save_html(vis, 'lda_visualization.html')  # Save the visualization locally

print("Visualization saved as 'lda_visualization.html'")  # Notify that the file has been saved

"""Embedding Analysis

"""

import torch
import torchvision
from transformers import AutoTokenizer, AutoModel

print(f"Torch version: {torch.__version__}")
print(f"Torchvision version: {torchvision.__version__}")

tokenizer = AutoTokenizer.from_pretrained("dccuchile/bert-base-spanish-wwm-cased")
model = AutoModel.from_pretrained("dccuchile/bert-base-spanish-wwm-cased")
print("BETO loaded successfully!")

import pandas as pd
from tqdm import tqdm

# Load the CSV file
data = pd.read_csv('/content/Assignment 2/data/participants_beto_sentiment_processed_cleaned.csv')

# Function to generate sentence embeddings using BETO
def get_embedding(sentence):
    """
    Generate a sentence embedding using BETO.
    """
    # Tokenize the input sentence
    inputs = tokenizer(sentence, return_tensors="pt", padding=True, truncation=True, max_length=512)
    # Get the model output
    outputs = model(**inputs)
    # Average the token embeddings to create a single vector for the sentence
    return outputs.last_hidden_state.mean(dim=1).detach().numpy()

# Add a progress bar to monitor the process
tqdm.pandas(desc="Generating embeddings")

# Generate embeddings for all texts in the 'Cleaned_for_Beto' column
data['Embedding'] = data['Cleaned_for_Beto'].progress_apply(lambda x: get_embedding(str(x)))

# Verify that embeddings are generated
print(data['Embedding'].head())

# Save the resulting DataFrame to a CSV file (optional)
data.to_csv('/content/processed_with_embeddings.csv', index=False)
print("Data with embeddings saved to '/content/processed_with_embeddings.csv'")

# Save the DataFrame with embeddings to a JSON file
data.to_json('/content/processed_with_embeddings_participants.json', orient='records', lines=True)

print("Data with embeddings saved to '/content/processed_with_embeddings_participants.json'")

# Load the data from the JSON file
data_with_embeddings = pd.read_json('/content/Assignment 2/data/processed_with_embeddings_participants.json', lines=True)

# Verify that the embeddings were loaded correctly
print(data_with_embeddings['Embedding'].head())

# Print one example of an embedding
print(data_with_embeddings['Embedding'].iloc[0])  # This shows the embedding for the first row

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Example: Compute similarity between two embeddings
embedding_1 = np.array(data_with_embeddings['Embedding'].iloc[0])  # Convert to NumPy array
embedding_2 = np.array(data_with_embeddings['Embedding'].iloc[1])  # Convert to NumPy array

# Reshape embeddings to 2D arrays
embedding_1 = embedding_1.reshape(1, -1)
embedding_2 = embedding_2.reshape(1, -1)

# Compute cosine similarity
similarity = cosine_similarity(embedding_1, embedding_2)
print(f"Similarity between text 1 and text 2: {similarity[0][0]}")

# Create an embedding matrix from the dataset
embedding_matrix = np.vstack(data_with_embeddings['Embedding'].apply(np.array))

# Compute the pairwise cosine similarity matrix
similarity_matrix = cosine_similarity(embedding_matrix)

# Save the similarity matrix to a CSV file
similarity_df = pd.DataFrame(similarity_matrix, columns=data_with_embeddings.index, index=data_with_embeddings.index)
similarity_df.to_csv('/content/similarity_matrix.csv')

print("Similarity matrix saved to '/content/similarity_matrix.csv'")

import seaborn as sns
import matplotlib.pyplot as plt

# Plot the similarity matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(similarity_matrix, cmap='viridis', xticklabels=False, yticklabels=False)
plt.title("Cosine Similarity Heatmap")
plt.show()

"""Clusterring topics

"""

import pandas as pd
import numpy as np

# Load the CSV file
original_data = pd.read_csv('/content/processed_with_embeddings_participants.csv')

# Display a sample of the 'Embedding' column
print("Sample of 'Embedding' column before processing:")
print(original_data['Embedding'].head(10))

# Clean the 'Embedding' column and convert it to numeric arrays
def parse_embedding(value):
    try:
        # Remove unwanted characters like \n and extra spaces
        value = value.replace('\n', '').replace('[', '').replace(']', '').strip()
        # Convert the cleaned string into a list of floats
        return np.array([float(x) for x in value.split()])
    except Exception as e:
        print(f"Error processing value: {value}, Error: {e}")
        return None

# Apply the parsing function
print("Parsing the 'Embedding' column...")
original_data['Embedding'] = original_data['Embedding'].apply(parse_embedding)

# Remove rows with invalid embeddings
valid_data = original_data[original_data['Embedding'].notnull()]

# Verify the cleaned data
print(f"Number of valid rows after parsing: {len(valid_data)}")
if len(valid_data) > 0:
    print("Sample of cleaned 'Embedding' column:")
    print(valid_data['Embedding'].head(10))

    # Convert the column to a matrix
    embeddings = np.vstack(valid_data['Embedding'])
    print(f"Embeddings shape: {embeddings.shape}")
else:
    print("No valid embeddings found after parsing.")

from sklearn.cluster import KMeans

# Define the number of clusters
num_clusters = 20  # Adjust this number based on your needs

# Apply KMeans clustering
print("Applying KMeans clustering...")
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
valid_data['Cluster'] = kmeans.fit_predict(embeddings)

# Save the results to a CSV file
output_path = '/content/processed_with_clusters.csv'
valid_data.to_csv(output_path, index=False)

print(f"Clustered data saved to: {output_path}")

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans

# Load the dataset
input_path = "/content/processed_with_clusters.csv"
output_path = "/content/reclustered_data.csv"

# Read the CSV file
data = pd.read_csv(input_path)

# Define the clusters we want to keep
relevant_clusters = [0, 1, 18, 19]

# Filter the dataset to include only relevant clusters
filtered_data = data[data['Cluster'].isin(relevant_clusters)].copy()

# Check how the Embedding column is stored
print("First few embeddings before conversion:", filtered_data['Embedding'].head())

# Convert the 'Embedding' column from space-separated string to NumPy array
filtered_data['Embedding'] = filtered_data['Embedding'].apply(
    lambda x: np.fromstring(x.strip("[]"), sep=" ")
)

# Verify the conversion
print("First few embeddings after conversion:", filtered_data['Embedding'].head())

# Convert list of embeddings into a structured DataFrame
embeddings = np.vstack(filtered_data['Embedding'].values)

# Define the number of new clusters (adjust as needed)
num_clusters = 20  # You can change this based on your needs

# Apply KMeans clustering
print("Applying KMeans clustering on the filtered data...")
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
filtered_data['New_Cluster'] = kmeans.fit_predict(embeddings)

# Save the re-clustered data while keeping all original columns
filtered_data.to_csv(output_path, index=False)

print(f"Re-clustered data saved to: {output_path}")

import pandas as pd
import numpy as np
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

# Load the re-clustered data
reclustered_data = pd.read_csv("/content/reclustered_data.csv")

# Extract embeddings from the re-clustered data
embeddings = np.vstack(reclustered_data['Embedding'].apply(
    lambda x: np.fromstring(x.strip("[]"), sep=" ")
))

# Reduce embeddings to 2D using t-SNE
print("Reducing dimensionality with t-SNE...")
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
reduced_embeddings = tsne.fit_transform(embeddings)

# Add reduced dimensions to the re-clustered DataFrame
reclustered_data['Dim1'] = reduced_embeddings[:, 0]
reclustered_data['Dim2'] = reduced_embeddings[:, 1]

# Plot the clusters with a more distinguishable color palette
plt.figure(figsize=(10, 8))
sns.scatterplot(
    x='Dim1',
    y='Dim2',
    hue='New_Cluster',  # Use the new cluster column
    palette='tab20',
    data=reclustered_data,  # Use the re-clustered data
    legend='full'
)
plt.title("Cluster Visualization with t-SNE (Reclustered Data)")
plt.xlabel("Dimension 1")
plt.ylabel("Dimension 2")
plt.legend(title="Cluster", loc='upper right')
plt.show()

from sklearn.manifold import TSNE
import plotly.express as px

# ... (previous code) ...

# Load the re-clustered data
reclustered_data = pd.read_csv("/content/reclustered_data.csv")

# Extract embeddings from the re-clustered data
embeddings = np.vstack(reclustered_data['Embedding'].apply(
    lambda x: np.fromstring(x.strip("[]"), sep=" ")
))

# Reduce embeddings to 3D using t-SNE
print("Reducing dimensionality with t-SNE to 3D...")
tsne_3d = TSNE(n_components=3, random_state=42, perplexity=30)
reduced_embeddings_3d = tsne_3d.fit_transform(embeddings)

# Add reduced dimensions to the reclustered_data DataFrame
reclustered_data['Dim1'] = reduced_embeddings_3d[:, 0]
reclustered_data['Dim2'] = reduced_embeddings_3d[:, 1]
reclustered_data['Dim3'] = reduced_embeddings_3d[:, 2]

# Create a 3D scatter plot using Plotly
fig = px.scatter_3d(
    reclustered_data,  # Use reclustered_data for plotting
    x='Dim1',
    y='Dim2',
    z='Dim3',
    color='New_Cluster',  # Use the 'New_Cluster' column
    hover_data=['Cleaned_for_Beto'],  # Add hover data
    title="3D Cluster Visualization with t-SNE (Reclustered Data)",
    labels={'New_Cluster': 'Cluster'},  # Update label
    color_discrete_sequence='tab20'
)

# Show the interactive 3D plot
fig.show()

# ... (rest of the code) ...

# Save the 3D plot as an HTML file
fig.write_html('/content/cluster_visualization_3d.html')

print("3D visualization saved as 'cluster_visualization_3d.html'")

import shutil
from google.colab import files

# Define the source folder and the output zip file
source_folder = '/content/Assignment 2/data'
output_zip = '/content/Assignment_2_data.zip'

# Compress the folder into a zip file
shutil.make_archive(output_zip.replace('.zip', ''), 'zip', source_folder)

# Download the zip file
files.download(output_zip)



